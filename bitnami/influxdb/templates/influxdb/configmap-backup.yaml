{{- if .Values.backup.enabled }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "common.names.fullname" . }}-backup
  labels:
    {{- include "common.labels.standard" . | nindent 4 }}
    app.kubernetes.io/component: influxdb
    {{- if .Values.commonLabels }}
    {{- include "common.tplvalues.render" ( dict "value" .Values.commonLabels "context" $ ) | nindent 4 }}
    {{- end }}
  {{- if .Values.commonAnnotations }}
  annotations: {{- include "common.tplvalues.render" ( dict "value" .Values.commonAnnotations "context" $ ) | nindent 4 }}
  {{- end }}
data:
  backup.sh: |-
    #!/bin/bash

    set -e

    . /opt/bitnami/scripts/libinfluxdb.sh

    DATE="$(date +%Y%m%d_%H%M%S)"
    BRANCH="$(influxdb_branch)"

    host="{{ include "common.names.fullname" . }}.{{ .Release.Namespace }}.svc"

    get_orgs() {
      INFLUX_TOKEN="${INFLUXDB_ADMIN_USER_TOKEN}" influx --host "http://${host}:{{ .Values.influxdb.service.port }}" org list 2> /dev/null | grep -v 'ID' | awk -F '\t' 'BEGIN{ORS=" "} {print $2}'
    }

    get_databases() {
        local org_name="${1:-}"
        if [[ "${BRANCH}" = "1" ]]; then
            influx -username "{{ .Values.auth.admin.username }}" -password "${INFLUXDB_ADMIN_USER_PASSWORD}" -host "${host}" -port {{ .Values.influxdb.service.port }} -execute 'SHOW DATABASES' | sed -e '1,3d'
        else
            INFLUX_TOKEN="${INFLUXDB_ADMIN_USER_TOKEN}" influx --host "http://${host}:{{ .Values.influxdb.service.port }}" bucket list --org "${org_name}" 2> /dev/null | grep -v 'ID' | awk -F '\t' 'BEGIN{ORS=" "} {print $2}'
        fi
    }

    if [[ "${BRANCH}" = "1" ]]; then
        for DATABASE in $(get_databases); do
            echo "backuping ${DATABASE} db to {{ .Values.backup.directory.local }}/${DATABASE}"
            mkdir -p {{ .Values.backup.directory.local }}/${DATABASE}

            influxd backup -host "${host}" -port {{ .Values.influxdb.service.port }} -portable -db ${DATABASE} {{ .Values.backup.directory.local }}/"${DATABASE}/${DATE}"
        done

        echo "deleting old backups"
        find {{ .Values.backup.directory.local }} -mindepth 2 -maxdepth 2 -not -name ".snapshot" -not -name "lost+found" -type d -mtime +{{ .Values.backup.retentionDays }} -exec rm -r {} \;
    else
        for ORG in $(get_orgs); do
            for BUCKET in $(get_databases "${ORG}"); do
                backup_dir="{{ .Values.backup.directory.local }}/${ORG}/${BUCKET}"
                echo "backuping ${BUCKET} bucket to ${backup_dir}"
                mkdir -p "${backup_dir}"

                INFLUX_TOKEN="${INFLUXDB_ADMIN_USER_TOKEN}" influx --host "http://${host}:{{ .Values.influxdb.service.port }}" backup --bucket "${BUCKET}" "${backup_dir}/${DATE}"
            done
        done

        echo "deleting old backups"
        find {{ .Values.backup.directory.local }} -mindepth 3 -maxdepth 3 -not -name ".snapshot" -not -name "lost+found" -type d -mtime +{{ .Values.backup.retentionDays }} -exec rm -r {} \;
    fi
  upload.sh: |-
    #!/bin/sh

    set -e

    # Google Cloud Storage
    if [[ {{ .Values.backup.uploadProviders.googlecloudstorage.enabled }} == "true" ]]; then
      rclone config create remote googlecloudstorage \
        client_id {{ .Values.backup.uploadProviders.googlecloudstorage.clientId }} \
        client_secret {{ .Values.backup.uploadProviders.googlecloudstorage.clientSecret }} \
        project_number {{ .Values.backup.uploadProviders.googlecloudstorage.projectNum }} \
        object_acl {{ .Values.backup.uploadProviders.googlecloudstorage.objectACL }} \
        bucket_acl {{ .Values.backup.uploadProviders.googlecloudstorage.bucketACL }} \
        location {{ .Values.backup.uploadProviders.googlecloudstorage.location }} \
        storage_class {{ .Values.backup.uploadProviders.googlecloudstorage.storageClass }} \
        token {{ .Values.backup.uploadProviders.googlecloudstorage.token }} \
        token_url {{ .Values.backup.uploadProviders.googlecloudstorage.tokenUrl }} \
        auth_url {{ .Values.backup.uploadProviders.googlecloudstorage.authUrl }}
      rclone sync {{ .Values.backup.directory.local }} googlecloudstorage:{{ .Values.backup.directory.remote }} --progress
    fi

    # Mega.nz
    if [[ {{ .Values.backup.uploadProviders.mega.enabled }} == "true" ]]; then
      rclone config create remote mega \
        user {{ .Values.backup.uploadProviders.mega.user }} \
        pass {{ .Values.backup.uploadProviders.mega.password }} \
        --obscure
      rclone sync {{ .Values.backup.directory.local }} mega:{{ .Values.backup.directory.remote }} --progress
    fi

    # Microsoft Azure
    if [[ {{ .Values.backup.uploadProviders.azureblob.enabled }} == "true" ]]; then
      rclone config create remote azureblob \
        endpoint {{ .Values.backup.uploadProviders.azureblob.endpoint }} \
        account {{ .Values.backup.uploadProviders.azureblob.account }} \
        key {{ .Values.backup.uploadProviders.azureblob.blobKey }} \
        sas_url {{ .Values.backup.uploadProviders.azureblob.sasUrl }}

      rclone sync {{ .Values.backup.directory.local }} azureblob:{{ .Values.backup.directory.remote }} --progress
    fi

    # S3
    if [[ {{ .Values.backup.uploadProviders.s3.enabled }} == "true" ]]; then
      rclone config create remote s3 \
        provider {{ .Values.backup.uploadProviders.s3.provider }} \
        access_key_id {{ .Values.backup.uploadProviders.s3.accessKey }} \
        secret_access_key {{ .Values.backup.uploadProviders.s3.secretkey }} \
        region {{ .Values.backup.uploadProviders.s3.region }} \
        endpoint {{ .Values.backup.uploadProviders.s3.endpoint }} \
        force_path_style {{ .Values.backup.uploadProviders.s3.forcePathStyle }}

      rclone sync {{ .Values.backup.directory.local }} s3:{{ .Values.backup.directory.remote }} --progress
    fi
{{ end }}
