---
# Source: milvus/charts/etcd/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: myml2-etcd
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: etcd
    app.kubernetes.io/version: 3.5.10
    helm.sh/chart: etcd-9.6.0
    app.kubernetes.io/component: etcd
spec:
  minAvailable: 51%
  selector:
    matchLabels:
      app.kubernetes.io/instance: myml2
      app.kubernetes.io/name: etcd
      app.kubernetes.io/component: etcd
---
# Source: milvus/charts/kafka/templates/rbac/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: myml2-kafka
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.6.0
    helm.sh/chart: kafka-26.3.1
    app.kubernetes.io/component: kafka
automountServiceAccountToken: true
---
# Source: milvus/charts/minio/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: myml2-minio
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2023.11.1
    helm.sh/chart: minio-12.9.0
automountServiceAccountToken: true
secrets:
  - name: myml2-minio
---
# Source: milvus/charts/etcd/templates/token-secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: myml2-etcd-jwt-token
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: etcd
    app.kubernetes.io/version: 3.5.10
    helm.sh/chart: etcd-9.6.0
type: Opaque
data:
  jwt-token.pem: "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"
---
# Source: milvus/charts/kafka/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: myml2-kafka-user-passwords
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.6.0
    helm.sh/chart: kafka-26.3.1
type: Opaque
data:
  client-passwords: "VDh1U1prVHZGRw=="
  system-user-password: "VDh1U1prVHZGRw=="
  inter-broker-password: "Q3pScmxadDU1eg=="
  controller-password: "S3pmaFplTnlvZw=="
---
# Source: milvus/charts/kafka/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: myml2-kafka-kraft-cluster-id
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.6.0
    helm.sh/chart: kafka-26.3.1
type: Opaque
data:
  kraft-cluster-id: "dTF4YTBYTjhPNk1pOXlnUTRCeTlhaQ=="
---
# Source: milvus/charts/minio/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: myml2-minio
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2023.11.1
    helm.sh/chart: minio-12.9.0
type: Opaque
data:
  root-user: "YWRtaW4="
  root-password: "Z3Z2andqVHdKYg=="
---
# Source: milvus/charts/kafka/templates/controller-eligible/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myml2-kafka-controller-configuration
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.6.0
    helm.sh/chart: kafka-26.3.1
    app.kubernetes.io/component: controller-eligible
    app.kubernetes.io/part-of: kafka
data:
  server.properties: |-
    # Listeners configuration
    listeners=CLIENT://:9092,INTERNAL://:9094,CONTROLLER://:9093
    advertised.listeners=CLIENT://advertised-address-placeholder:9092,INTERNAL://advertised-address-placeholder:9094
    listener.security.protocol.map=CLIENT:SASL_PLAINTEXT,INTERNAL:SASL_PLAINTEXT,CONTROLLER:SASL_PLAINTEXT
    # KRaft process roles
    process.roles=controller,broker
    #node.id=
    controller.listener.names=CONTROLLER
    controller.quorum.voters=0@myml2-kafka-controller-0.myml2-kafka-controller-headless.default.svc.cluster.local:9093
    # Kraft Controller listener SASL settings
    sasl.mechanism.controller.protocol=PLAIN
    listener.name.controller.sasl.enabled.mechanisms=PLAIN
    listener.name.controller.plain.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="controller_user" password="controller-password-placeholder" user_controller_user="controller-password-placeholder";
    log.dir=/bitnami/kafka/data
    sasl.enabled.mechanisms=PLAIN
    # Interbroker configuration
    inter.broker.listener.name=INTERNAL
    sasl.mechanism.inter.broker.protocol=PLAIN
    # Listeners SASL JAAS configuration
    listener.name.client.plain.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required user_user="password-placeholder-0";
    listener.name.internal.plain.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="inter_broker_user" password="interbroker-password-placeholder" user_inter_broker_user="interbroker-password-placeholder" user_user="password-placeholder-0";
    # End of SASL JAAS configuration
    offsets.topic.replication.factor=1
---
# Source: milvus/charts/kafka/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myml2-kafka-scripts
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.6.0
    helm.sh/chart: kafka-26.3.1
data:
  kafka-init.sh: |-
    #!/bin/bash

    set -o errexit
    set -o nounset
    set -o pipefail

    error(){
      local message="${1:?missing message}"
      echo "ERROR: ${message}"
      exit 1
    }

    retry_while() {
        local -r cmd="${1:?cmd is missing}"
        local -r retries="${2:-12}"
        local -r sleep_time="${3:-5}"
        local return_value=1

        read -r -a command <<< "$cmd"
        for ((i = 1 ; i <= retries ; i+=1 )); do
            "${command[@]}" && return_value=0 && break
            sleep "$sleep_time"
        done
        return $return_value
    }

    replace_in_file() {
        local filename="${1:?filename is required}"
        local match_regex="${2:?match regex is required}"
        local substitute_regex="${3:?substitute regex is required}"
        local posix_regex=${4:-true}

        local result

        # We should avoid using 'sed in-place' substitutions
        # 1) They are not compatible with files mounted from ConfigMap(s)
        # 2) We found incompatibility issues with Debian10 and "in-place" substitutions
        local -r del=$'\001' # Use a non-printable character as a 'sed' delimiter to avoid issues
        if [[ $posix_regex = true ]]; then
            result="$(sed -E "s${del}${match_regex}${del}${substitute_regex}${del}g" "$filename")"
        else
            result="$(sed "s${del}${match_regex}${del}${substitute_regex}${del}g" "$filename")"
        fi
        echo "$result" > "$filename"
    }

    kafka_conf_set() {
        local file="${1:?missing file}"
        local key="${2:?missing key}"
        local value="${3:?missing value}"

        # Check if the value was set before
        if grep -q "^[#\\s]*$key\s*=.*" "$file"; then
            # Update the existing key
            replace_in_file "$file" "^[#\\s]*${key}\s*=.*" "${key}=${value}" false
        else
            # Add a new key
            printf '\n%s=%s' "$key" "$value" >>"$file"
        fi
    }

    replace_placeholder() {
      local placeholder="${1:?missing placeholder value}"
      local password="${2:?missing password value}"
      sed -i "s/$placeholder/$password/g" "$KAFKA_CONFIG_FILE"
    }

    append_file_to_kafka_conf() {
        local file="${1:?missing source file}"
        local conf="${2:?missing kafka conf file}"

        cat "$1" >> "$2"
    }

    configure_external_access() {
      # Configure external hostname
      if [[ -f "/shared/external-host.txt" ]]; then
        host=$(cat "/shared/external-host.txt")
      elif [[ -n "${EXTERNAL_ACCESS_HOST:-}" ]]; then
        host="$EXTERNAL_ACCESS_HOST"
      elif [[ -n "${EXTERNAL_ACCESS_HOSTS_LIST:-}" ]]; then
        read -r -a hosts <<<"$(tr ',' ' ' <<<"${EXTERNAL_ACCESS_HOSTS_LIST}")"
        host="${hosts[$POD_ID]}"
      elif [[ "$EXTERNAL_ACCESS_HOST_USE_PUBLIC_IP" =~ ^(yes|true)$ ]]; then
        host=$(curl -s https://ipinfo.io/ip)
      else
        error "External access hostname not provided"
      fi

      # Configure external port
      if [[ -f "/shared/external-port.txt" ]]; then
        port=$(cat "/shared/external-port.txt")
      elif [[ -n "${EXTERNAL_ACCESS_PORT:-}" ]]; then
        if [[ "${EXTERNAL_ACCESS_PORT_AUTOINCREMENT:-}" =~ ^(yes|true)$ ]]; then
          port="$((EXTERNAL_ACCESS_PORT + POD_ID))"
        else
          port="$EXTERNAL_ACCESS_PORT"
        fi
      elif [[ -n "${EXTERNAL_ACCESS_PORTS_LIST:-}" ]]; then
        read -r -a ports <<<"$(tr ',' ' ' <<<"${EXTERNAL_ACCESS_PORTS_LIST}")"
        port="${ports[$POD_ID]}"
      else
        error "External access port not provided"
      fi
      # Configure Kafka advertised listeners
      sed -i -E "s|^(advertised\.listeners=\S+)$|\1,EXTERNAL://${host}:${port}|" "$KAFKA_CONFIG_FILE"
    }
    configure_kafka_sasl() {

      # Replace placeholders with passwords
      replace_placeholder "interbroker-password-placeholder" "$KAFKA_INTER_BROKER_PASSWORD"
      replace_placeholder "controller-password-placeholder" "$KAFKA_CONTROLLER_PASSWORD"
      read -r -a passwords <<<"$(tr ',;' ' ' <<<"${KAFKA_CLIENT_PASSWORDS:-}")"
      for ((i = 0; i < ${#passwords[@]}; i++)); do
          replace_placeholder "password-placeholder-${i}" "${passwords[i]}"
      done
    }

    export KAFKA_CONFIG_FILE=/config/server.properties
    cp /configmaps/server.properties $KAFKA_CONFIG_FILE

    # Get pod ID and role, last and second last fields in the pod name respectively
    POD_ID=$(echo "$MY_POD_NAME" | rev | cut -d'-' -f 1 | rev)
    POD_ROLE=$(echo "$MY_POD_NAME" | rev | cut -d'-' -f 2 | rev)

    # Configure node.id and/or broker.id
    if [[ -f "/bitnami/kafka/data/meta.properties" ]]; then
        if grep -q "broker.id" /bitnami/kafka/data/meta.properties; then
          ID="$(grep "broker.id" /bitnami/kafka/data/meta.properties | awk -F '=' '{print $2}')"
          kafka_conf_set "$KAFKA_CONFIG_FILE" "node.id" "$ID"
        else
          ID="$(grep "node.id" /bitnami/kafka/data/meta.properties | awk -F '=' '{print $2}')"
          kafka_conf_set "$KAFKA_CONFIG_FILE" "node.id" "$ID"
        fi
    else
        ID=$((POD_ID + KAFKA_MIN_ID))
        kafka_conf_set "$KAFKA_CONFIG_FILE" "node.id" "$ID"
    fi
    replace_placeholder "advertised-address-placeholder" "${MY_POD_NAME}.myml2-kafka-${POD_ROLE}-headless.default.svc.cluster.local"
    if [[ "${EXTERNAL_ACCESS_ENABLED:-false}" =~ ^(yes|true)$ ]]; then
      configure_external_access
    fi
    configure_kafka_sasl
    if [ -f /secret-config/server-secret.properties ]; then
      append_file_to_kafka_conf /secret-config/server-secret.properties $KAFKA_CONFIG_FILE
    fi
---
# Source: milvus/charts/minio/templates/provisioning-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myml2-minio-provisioning
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2023.11.1
    helm.sh/chart: minio-12.9.0
    app.kubernetes.io/component: minio-provisioning
data:
---
# Source: milvus/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myml2-milvus
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
data:
  00_milvus_default.yaml: |
    # etcd configuration
    etcd:
      endpoints:
        - http://myml2-etcd-0.myml2-etcd-headless:2379
        - http://myml2-etcd-1.myml2-etcd-headless:2379
        - http://myml2-etcd-2.myml2-etcd-headless:2379
    metastore:
      type: etcd
    
    # S3 configuration
    minio:
      address: myml2-minio
      port: 80
      accessKeyID: "{{ MILVUS_S3_ACCESS_ID }}"
      secretAccessKey: "{{ MILVUS_S3_SECRET_ACCESS_KEY }}"
      useSSL: false
      bucketName: milvus
      rootPath: file
      useIAM: false
    
    # Kafka configuration
    kafka:
      brokerList:
        - myml2-kafka-controller-0.myml2-kafka-controller-headless:9092
      securityProtocol: SASL_PLAINTEXT
      saslMechanisms: PLAIN
      saslUsername: user
      saslPassword: "{{ MILVUS_KAFKA_PASSWORD }}"
    
    # Data coordinator
    dataCoord:
      address: myml2-milvus-data-coordinator
      port: 19530
    
    # Root coordinator
    rootCoord:
      address: myml2-milvus-root-coordinator
      port: 19530
    
    # Index coordinator
    indexCoord:
      address: myml2-milvus-index-coordinator
      port: 19530
    
    # Query coordinator
    queryCoord:
      address: myml2-milvus-query-coordinator
      port: 19530
    
    # Data node
    dataNode:
      port: 19530
    
    # Index node
    indexNode:
      port: 19530
    
    # Query node
    queryNode:
      port: 19530
    
    proxy:
      port: 19530
      accessLog:
        localPath: /dev
        filename: stdout
      http:
        enabled: true
    
    # Log configuration
    log:
      level: info
      stdout: true
    
    # Common configuration
    common:
      storageType: minio
      security:
        authorizationEnabled: false
---
# Source: milvus/templates/data-coordinator/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myml2-milvus-data-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
data:
  03_data_coordinator_default.yaml: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    dataCoord:
      port: 19530
      enableActiveStandby: true
---
# Source: milvus/templates/data-node/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myml2-milvus-data-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
data:
  03_data_node_default.yaml: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    dataNode:
      port: 19530
      enableDisk: true
---
# Source: milvus/templates/index-coordinator/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myml2-milvus-index-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
data:
  03_index_coordinator_default.yaml: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    indexCoord:
      port: 19530
      enableActiveStandby: true
---
# Source: milvus/templates/index-node/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myml2-milvus-index-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
data:
  03_index_node_default.yaml: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    indexNode:
      port: 19530
      enableDisk: true
---
# Source: milvus/templates/log-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myml2-milvus-log
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
data:
  glog.conf: |
    # if true, only log to stdout
    --logtostdout=true
    --logtostderr=false
    --alsologtostderr=false
    # `INFO``, ``WARNING``, ``ERROR``, and ``FATAL`` are 0, 1, 2, and 3
    --minloglevel=0
    --log_dir=/var/lib/milvus/logs/
    # MB 
    --max_log_size=200
    --stop_logging_if_full_disk=true
---
# Source: milvus/templates/proxy/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myml2-milvus-proxy
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
data:
  03_index_node_default.yaml: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    proxy:
      port: 19530
      internalPort: 19529
---
# Source: milvus/templates/query-coordinator/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myml2-milvus-query-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
data:
  03_query_coordinator_default.yaml: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    queryCoord:
      port: 19530
      enableActiveStandby: true
---
# Source: milvus/templates/query-node/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myml2-milvus-query-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
data:
  03_query_node_default.yaml: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    queryNode:
      port: 19530
      enableDisk: true
---
# Source: milvus/templates/root-coordinator/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myml2-milvus-root-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
data:
  03_root_coordinator_default.yaml: |
    # Override the port for internal binding (the external components will use the service port defined in milvus.defaultConfig)
    rootCoord:
      port: 19530
      enableActiveStandby: true
---
# Source: milvus/charts/minio/templates/pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: myml2-minio
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2023.11.1
    helm.sh/chart: minio-12.9.0
spec:
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "8Gi"
---
# Source: milvus/charts/etcd/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: myml2-etcd-headless
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: etcd
    app.kubernetes.io/version: 3.5.10
    helm.sh/chart: etcd-9.6.0
    app.kubernetes.io/component: etcd
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: client
      port: 2379
      targetPort: client
    - name: peer
      port: 2380
      targetPort: peer
  selector:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/name: etcd
    app.kubernetes.io/component: etcd
---
# Source: milvus/charts/etcd/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: myml2-etcd
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: etcd
    app.kubernetes.io/version: 3.5.10
    helm.sh/chart: etcd-9.6.0
    app.kubernetes.io/component: etcd
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: "client"
      port: 2379
      targetPort: client
      nodePort: null
    - name: "peer"
      port: 2380
      targetPort: peer
      nodePort: null
  selector:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/name: etcd
    app.kubernetes.io/component: etcd
---
# Source: milvus/charts/kafka/templates/controller-eligible/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: myml2-kafka-controller-headless
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.6.0
    helm.sh/chart: kafka-26.3.1
    app.kubernetes.io/component: controller-eligible
    app.kubernetes.io/part-of: kafka
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: tcp-interbroker
      port: 9094
      protocol: TCP
      targetPort: interbroker
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: client
    - name: tcp-controller
      protocol: TCP
      port: 9093
      targetPort: controller
  selector:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/name: kafka
    app.kubernetes.io/component: controller-eligible
    app.kubernetes.io/part-of: kafka
---
# Source: milvus/charts/kafka/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: myml2-kafka
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.6.0
    helm.sh/chart: kafka-26.3.1
    app.kubernetes.io/component: kafka
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: client
      nodePort: null
  selector:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/name: kafka
    app.kubernetes.io/part-of: kafka
---
# Source: milvus/charts/minio/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myml2-minio
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2023.11.1
    helm.sh/chart: minio-12.9.0
spec:
  type: ClusterIP
  ports:
    - name: minio-api
      port: 80
      targetPort: minio-api
      nodePort: null
    - name: minio-console
      port: 9001
      targetPort: minio-console
      nodePort: null
  selector:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/name: minio
---
# Source: milvus/templates/attu/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myml2-milvus-attu
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.3.2
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: attu
spec:
  type: LoadBalancer
  sessionAffinity: None
  externalTrafficPolicy: "Cluster"
  
  loadBalancerSourceRanges: []
  
  ports:
    - name: http
      port: 80
      targetPort: http
      protocol: TCP
  selector:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/name: milvus
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: attu
---
# Source: milvus/templates/data-coordinator/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myml2-milvus-data-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: data-coordinator
spec:
  type: ClusterIP
  sessionAffinity: None
  
  ports:
    - name: grpc
      port: 19530
      targetPort: grpc
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/name: milvus
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: data-coordinator
---
# Source: milvus/templates/data-node/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myml2-milvus-data-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: data-node
spec:
  type: ClusterIP
  sessionAffinity: None
  
  ports:
    - name: grpc
      port: 19530
      targetPort: grpc
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/name: milvus
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: data-node
---
# Source: milvus/templates/index-coordinator/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myml2-milvus-index-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: index-coordinator
spec:
  type: ClusterIP
  sessionAffinity: None
  
  ports:
    - name: grpc
      port: 19530
      targetPort: grpc
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/name: milvus
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: index-coordinator
---
# Source: milvus/templates/index-node/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myml2-milvus-index-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: index-node
spec:
  type: ClusterIP
  sessionAffinity: None
  
  ports:
    - name: grpc
      port: 19530
      targetPort: grpc
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/name: milvus
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: index-node
---
# Source: milvus/templates/proxy/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myml2-milvus-proxy
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: proxy
spec:
  type: LoadBalancer
  sessionAffinity: None
  externalTrafficPolicy: "Cluster"
  
  loadBalancerSourceRanges: []
  
  ports:
    - name: grpc
      port: 19530
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/name: milvus
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: proxy
---
# Source: milvus/templates/query-coordinator/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myml2-milvus-query-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: query-coordinator
spec:
  type: ClusterIP
  sessionAffinity: None
  
  ports:
    - name: grpc
      port: 19530
      targetPort: grpc
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/name: milvus
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: query-coordinator
---
# Source: milvus/templates/query-node/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myml2-milvus-query-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: query-node
spec:
  type: ClusterIP
  sessionAffinity: None
  
  ports:
    - name: grpc
      port: 19530
      targetPort: grpc
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/name: milvus
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: query-node
---
# Source: milvus/templates/root-coordinator/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myml2-milvus-root-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: root-coordinator
spec:
  type: ClusterIP
  sessionAffinity: None
  
  ports:
    - name: grpc
      port: 19530
      targetPort: grpc
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/name: milvus
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: root-coordinator
---
# Source: milvus/charts/minio/templates/standalone/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myml2-minio
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2023.11.1
    helm.sh/chart: minio-12.9.0
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: myml2
      app.kubernetes.io/name: minio
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: myml2
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: minio
        app.kubernetes.io/version: 2023.11.1
        helm.sh/chart: minio-12.9.0
      annotations:
        checksum/credentials-secret: a15921c2c851bd4dea1be75546502e5fd5b624a3a6ae3e5aba98d0d8905e53e4
    spec:
      
      serviceAccountName: myml2-minio
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: myml2
                    app.kubernetes.io/name: minio
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      containers:
        - name: minio
          image: docker.io/bitnami/minio:2023.11.1-debian-11-r0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MINIO_SCHEME
              value: "http"
            - name: MINIO_FORCE_NEW_KEYS
              value: "no"
            - name: MINIO_ROOT_USER
              valueFrom:
                secretKeyRef:
                  name: myml2-minio
                  key: root-user
            - name: MINIO_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: myml2-minio
                  key: root-password
            - name: MINIO_DEFAULT_BUCKETS
              value: milvus
            - name: MINIO_BROWSER
              value: "on"
            - name: MINIO_PROMETHEUS_AUTH_TYPE
              value: "public"
            - name: MINIO_CONSOLE_PORT_NUMBER
              value: "9001"
          envFrom:
          ports:
            - name: minio-api
              containerPort: 9000
              protocol: TCP
            - name: minio-console
              containerPort: 9001
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /minio/health/live
              port: minio-api
              scheme: "HTTP"
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            tcpSocket:
              port: minio-api
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 5
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /bitnami/minio/data
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: myml2-minio
---
# Source: milvus/templates/attu/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myml2-milvus-attu
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.3.2
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: attu
spec:
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: myml2
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: attu
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: myml2
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: milvus
        app.kubernetes.io/version: 2.3.2
        helm.sh/chart: milvus-5.0.0
        app.kubernetes.io/part-of: milvus
        app.kubernetes.io/component: attu
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: myml2
                    app.kubernetes.io/name: milvus
                    app.kubernetes.io/component: attu
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
        - name: wait-for-proxy
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1 
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              check_proxy() {
                  local -r proxy_host="${1:-?missing proxy}"
                  if wait-for-port --timeout=5 --host=${proxy_host} --state=inuse 19530; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              host="myml2-milvus-proxy"
        
              echo "Checking connection to $host"
              if retry_while "check_proxy $host"; then
                  echo "Connected to $host"
              else
                  echo "Error connecting to $host"
                  exit 1
              fi
        
              echo "Connection success"
              exit 0
      containers:
        - name: attu
          image: docker.io/bitnami/attu:2.3.2-debian-11-r1
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          env:
            - name: MILVUS_URL
              value: http://myml2-milvus-proxy:19530
          envFrom:
          ports:
            - containerPort: 3000
              name: http
          resources:
            limits: {}
            requests: {}
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /
              port: http
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /
              port: http
          volumeMounts:
            - name: tmp
              mountPath: /tmp
            - name: tmp-npm
              mountPath: /.npm
            - name: tmp-yarn
              mountPath: /.yarn
            - name: tmp-yarn-cache
              mountPath: /.cache/yarn
      volumes:
        - name: tmp-yarn
          emptyDir: {}
        - name: tmp-yarn-cache
          emptyDir: {}
        - name: tmp-npm
          emptyDir: {}
        - name: tmp
          emptyDir: {}
---
# Source: milvus/templates/data-coordinator/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myml2-milvus-data-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: data-coordinator
spec:
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: myml2
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: data-coordinator
  template:
    metadata:
      annotations:
        checksum/common-config: 1d718420d1d135b8a6c5de63a3432db00d66753ff5e7c2c437683a14be472202
        checksum/common-config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      labels:
        app.kubernetes.io/instance: myml2
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: milvus
        app.kubernetes.io/version: 2.2.14
        helm.sh/chart: milvus-5.0.0
        app.kubernetes.io/part-of: milvus
        app.kubernetes.io/component: data-coordinator
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: myml2
                    app.kubernetes.io/name: milvus
                    app.kubernetes.io/component: data-coordinator
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
        - name: wait-for-etcd
          image: docker.io/bitnami/os-shell:11-debian-11-r91
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              etcd_hosts=(
                "http://myml2-etcd:2379"
              )
        
              check_etcd() {
                  local -r etcd_host="${1:-?missing etcd}"
                  local params_cert=""
        
                  if echo $etcd_host | grep https; then
                     params_cert="--cacert /bitnami/milvus/conf/cert/etcd/client/ca.crt --cert /bitnami/milvus/conf/cert/etcd/client/tls.crt --key /bitnami/milvus/conf/cert/etcd/client/tls.key"
                  fi
                  if [ ! -z  ]; then
                    params_cert=$params_cert" --pass "
                  fi
                  if curl --max-time 5 "${etcd_host}/version" $params_cert | grep etcdcluster; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${etcd_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_etcd $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-kafka
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1 
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              kafka_hosts=(
                "myml2-kafka"
              )
        
              check_kafka() {
                  local -r kafka_host="${1:-?missing kafka}"
                  if wait-for-port --timeout=5 --host=${kafka_host} --state=inuse 9092; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${kafka_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_kafka $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-s3
          image: docker.io/bitnami/os-shell:11-debian-11-r91
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              check_s3() {
                  local -r s3_host="${1:-?missing s3}"
                  if curl --max-time 5 "${s3_host}" | grep "RequestId"; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              host="myml2-minio"
        
              echo "Checking connection to $host"
              if retry_while "check_s3 $host"; then
                echo "Connected to $host"
              else
                echo "Error connecting to $host"
                exit 1
              fi
        
              echo "Connection success"
              exit 0
        # This init container renders and merges the Milvus configuration files.
        # We need to use a volume because we're working with ReadOnlyRootFilesystem
        - name: prepare-milvus
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              # Copy logging configuration
              cp /bitnami/milvus/conf/log/glog.conf /bitnami/milvus/rendered-conf/glog.conf
              # Build final milvus.yaml with the sections of the different files
              find /bitnami/milvus/conf -type f -name *.yaml -print0 | sort -z | xargs -0 yq eval-all '. as $item ireduce ({}; . * $item )' /opt/bitnami/milvus/configs/milvus.yaml > /bitnami/milvus/rendered-conf/pre-render-config_00.yaml
              # HACK: In order to enable Kafka we need to remove all Pulsar settings from the configuration file
              # https://github.com/milvus-io/milvus/blob/master/configs/milvus.yaml#L110
              yq 'del(.pulsar)' /bitnami/milvus/rendered-conf/pre-render-config_00.yaml > /bitnami/milvus/rendered-conf/pre-render-config_01.yaml
              render-template /bitnami/milvus/rendered-conf/pre-render-config_01.yaml > /bitnami/milvus/rendered-conf/milvus.yaml
              rm /bitnami/milvus/rendered-conf/pre-render-config*
              chmod 644 /bitnami/milvus/rendered-conf/milvus.yaml
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MILVUS_KAFKA_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: myml2-kafka-user-passwords
                  key: system-user-password
            - name: MILVUS_S3_ACCESS_ID
              valueFrom:
                secretKeyRef:
                  name: myml2-minio
                  key: root-user
            - name: MILVUS_S3_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: myml2-minio
                  key: root-password
          envFrom:
          volumeMounts:
            - name: log-config
              mountPath: /bitnami/milvus/conf/log
            - name: config-common
              mountPath: /bitnami/milvus/conf/00_default
            - name: component-config-default
              mountPath: /bitnami/milvus/conf/02_component_default
            - name: tmp
              mountPath: /tmp
            - name: rendered-config
              mountPath: /bitnami/milvus/rendered-conf/
      containers:
        - name: milvus
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          args:
            - run
            - datacoord
          env:
            - name: METRICS_PORT
              value: "9091"
          envFrom:
          ports:
            - containerPort: 19530
              name: grpc
            - containerPort: 9091
              name: http-metrics
          resources:
            limits: {}
            requests: {}
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          volumeMounts:
            - name: rendered-config
              mountPath: /opt/bitnami/milvus/configs
            - name: tmp
              mountPath: /tmp
            - name: tmp-milvus
              mountPath: /opt/bitnami/milvus/tmp
              # We are using a s3 backend, so this data dir is temporary
            - name: tmp-data-milvus
              mountPath: /bitnami/milvus/data
      volumes:
        - name: tmp-data-milvus
          emptyDir: {}
        - name: tmp-milvus
          emptyDir: {}
        - name: tmp
          emptyDir: {}
        - name: log-config
          configMap:
            name: myml2-milvus-log
        - name: config-common
          configMap:
            name: myml2-milvus
        - name: component-config-default
          configMap:
            name: myml2-milvus-data-coordinator
        - name: rendered-config
          emptyDir: {}
---
# Source: milvus/templates/data-node/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myml2-milvus-data-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: data-node
spec:
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: myml2
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: data-node
  template:
    metadata:
      annotations:
        checksum/common-config: 1d718420d1d135b8a6c5de63a3432db00d66753ff5e7c2c437683a14be472202
        checksum/common-config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      labels:
        app.kubernetes.io/instance: myml2
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: milvus
        app.kubernetes.io/version: 2.2.14
        helm.sh/chart: milvus-5.0.0
        app.kubernetes.io/part-of: milvus
        app.kubernetes.io/component: data-node
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: myml2
                    app.kubernetes.io/name: milvus
                    app.kubernetes.io/component: data-node
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
        - name: wait-for-etcd
          image: docker.io/bitnami/os-shell:11-debian-11-r91
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              etcd_hosts=(
                "http://myml2-etcd:2379"
              )
        
              check_etcd() {
                  local -r etcd_host="${1:-?missing etcd}"
                  local params_cert=""
        
                  if echo $etcd_host | grep https; then
                     params_cert="--cacert /bitnami/milvus/conf/cert/etcd/client/ca.crt --cert /bitnami/milvus/conf/cert/etcd/client/tls.crt --key /bitnami/milvus/conf/cert/etcd/client/tls.key"
                  fi
                  if [ ! -z  ]; then
                    params_cert=$params_cert" --pass "
                  fi
                  if curl --max-time 5 "${etcd_host}/version" $params_cert | grep etcdcluster; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${etcd_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_etcd $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-kafka
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1 
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              kafka_hosts=(
                "myml2-kafka"
              )
        
              check_kafka() {
                  local -r kafka_host="${1:-?missing kafka}"
                  if wait-for-port --timeout=5 --host=${kafka_host} --state=inuse 9092; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${kafka_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_kafka $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-s3
          image: docker.io/bitnami/os-shell:11-debian-11-r91
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              check_s3() {
                  local -r s3_host="${1:-?missing s3}"
                  if curl --max-time 5 "${s3_host}" | grep "RequestId"; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              host="myml2-minio"
        
              echo "Checking connection to $host"
              if retry_while "check_s3 $host"; then
                echo "Connected to $host"
              else
                echo "Error connecting to $host"
                exit 1
              fi
        
              echo "Connection success"
              exit 0
        # This init container renders and merges the Milvus configuration files.
        # We need to use a volume because we're working with ReadOnlyRootFilesystem
        - name: prepare-milvus
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              # Copy logging configuration
              cp /bitnami/milvus/conf/log/glog.conf /bitnami/milvus/rendered-conf/glog.conf
              # Build final milvus.yaml with the sections of the different files
              find /bitnami/milvus/conf -type f -name *.yaml -print0 | sort -z | xargs -0 yq eval-all '. as $item ireduce ({}; . * $item )' /opt/bitnami/milvus/configs/milvus.yaml > /bitnami/milvus/rendered-conf/pre-render-config_00.yaml
              # HACK: In order to enable Kafka we need to remove all Pulsar settings from the configuration file
              # https://github.com/milvus-io/milvus/blob/master/configs/milvus.yaml#L110
              yq 'del(.pulsar)' /bitnami/milvus/rendered-conf/pre-render-config_00.yaml > /bitnami/milvus/rendered-conf/pre-render-config_01.yaml
              render-template /bitnami/milvus/rendered-conf/pre-render-config_01.yaml > /bitnami/milvus/rendered-conf/milvus.yaml
              rm /bitnami/milvus/rendered-conf/pre-render-config*
              chmod 644 /bitnami/milvus/rendered-conf/milvus.yaml
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MILVUS_KAFKA_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: myml2-kafka-user-passwords
                  key: system-user-password
            - name: MILVUS_S3_ACCESS_ID
              valueFrom:
                secretKeyRef:
                  name: myml2-minio
                  key: root-user
            - name: MILVUS_S3_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: myml2-minio
                  key: root-password
          envFrom:
          volumeMounts:
            - name: log-config
              mountPath: /bitnami/milvus/conf/log
            - name: config-common
              mountPath: /bitnami/milvus/conf/00_default
            - name: component-config-default
              mountPath: /bitnami/milvus/conf/02_component_default
            - name: tmp
              mountPath: /tmp
            - name: rendered-config
              mountPath: /bitnami/milvus/rendered-conf/
      containers:
        - name: milvus
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          args:
            - run
            - datanode
          env:
            - name: METRICS_PORT
              value: "9091"
          envFrom:
          ports:
            - containerPort: 19530
              name: grpc
            - containerPort: 9091
              name: http-metrics
          resources:
            limits: {}
            requests: {}
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          volumeMounts:
            - name: rendered-config
              mountPath: /opt/bitnami/milvus/configs
            - name: tmp
              mountPath: /tmp
            - name: tmp-milvus
              mountPath: /opt/bitnami/milvus/tmp
              # We are using a s3 backend, so this data dir is temporary
            - name: tmp-data-milvus
              mountPath: /bitnami/milvus/data
      volumes:
        - name: tmp-data-milvus
          emptyDir: {}
        - name: tmp-milvus
          emptyDir: {}
        - name: tmp
          emptyDir: {}
        - name: log-config
          configMap:
            name: myml2-milvus-log
        - name: config-common
          configMap:
            name: myml2-milvus
        - name: component-config-default
          configMap:
            name: myml2-milvus-data-node
        - name: rendered-config
          emptyDir: {}
---
# Source: milvus/templates/index-coordinator/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myml2-milvus-index-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: index-coordinator
spec:
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: myml2
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: index-coordinator
  template:
    metadata:
      annotations:
        checksum/common-config: 1d718420d1d135b8a6c5de63a3432db00d66753ff5e7c2c437683a14be472202
        checksum/common-config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      labels:
        app.kubernetes.io/instance: myml2
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: milvus
        app.kubernetes.io/version: 2.2.14
        helm.sh/chart: milvus-5.0.0
        app.kubernetes.io/part-of: milvus
        app.kubernetes.io/component: index-coordinator
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: myml2
                    app.kubernetes.io/name: milvus
                    app.kubernetes.io/component: index-coordinator
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
        - name: wait-for-etcd
          image: docker.io/bitnami/os-shell:11-debian-11-r91
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              etcd_hosts=(
                "http://myml2-etcd:2379"
              )
        
              check_etcd() {
                  local -r etcd_host="${1:-?missing etcd}"
                  local params_cert=""
        
                  if echo $etcd_host | grep https; then
                     params_cert="--cacert /bitnami/milvus/conf/cert/etcd/client/ca.crt --cert /bitnami/milvus/conf/cert/etcd/client/tls.crt --key /bitnami/milvus/conf/cert/etcd/client/tls.key"
                  fi
                  if [ ! -z  ]; then
                    params_cert=$params_cert" --pass "
                  fi
                  if curl --max-time 5 "${etcd_host}/version" $params_cert | grep etcdcluster; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${etcd_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_etcd $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-kafka
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1 
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              kafka_hosts=(
                "myml2-kafka"
              )
        
              check_kafka() {
                  local -r kafka_host="${1:-?missing kafka}"
                  if wait-for-port --timeout=5 --host=${kafka_host} --state=inuse 9092; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${kafka_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_kafka $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-s3
          image: docker.io/bitnami/os-shell:11-debian-11-r91
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              check_s3() {
                  local -r s3_host="${1:-?missing s3}"
                  if curl --max-time 5 "${s3_host}" | grep "RequestId"; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              host="myml2-minio"
        
              echo "Checking connection to $host"
              if retry_while "check_s3 $host"; then
                echo "Connected to $host"
              else
                echo "Error connecting to $host"
                exit 1
              fi
        
              echo "Connection success"
              exit 0
        # This init container renders and merges the Milvus configuration files.
        # We need to use a volume because we're working with ReadOnlyRootFilesystem
        - name: prepare-milvus
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              # Copy logging configuration
              cp /bitnami/milvus/conf/log/glog.conf /bitnami/milvus/rendered-conf/glog.conf
              # Build final milvus.yaml with the sections of the different files
              find /bitnami/milvus/conf -type f -name *.yaml -print0 | sort -z | xargs -0 yq eval-all '. as $item ireduce ({}; . * $item )' /opt/bitnami/milvus/configs/milvus.yaml > /bitnami/milvus/rendered-conf/pre-render-config_00.yaml
              # HACK: In order to enable Kafka we need to remove all Pulsar settings from the configuration file
              # https://github.com/milvus-io/milvus/blob/master/configs/milvus.yaml#L110
              yq 'del(.pulsar)' /bitnami/milvus/rendered-conf/pre-render-config_00.yaml > /bitnami/milvus/rendered-conf/pre-render-config_01.yaml
              render-template /bitnami/milvus/rendered-conf/pre-render-config_01.yaml > /bitnami/milvus/rendered-conf/milvus.yaml
              rm /bitnami/milvus/rendered-conf/pre-render-config*
              chmod 644 /bitnami/milvus/rendered-conf/milvus.yaml
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MILVUS_KAFKA_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: myml2-kafka-user-passwords
                  key: system-user-password
            - name: MILVUS_S3_ACCESS_ID
              valueFrom:
                secretKeyRef:
                  name: myml2-minio
                  key: root-user
            - name: MILVUS_S3_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: myml2-minio
                  key: root-password
          envFrom:
          volumeMounts:
            - name: log-config
              mountPath: /bitnami/milvus/conf/log
            - name: config-common
              mountPath: /bitnami/milvus/conf/00_default
            - name: component-config-default
              mountPath: /bitnami/milvus/conf/02_component_default
            - name: tmp
              mountPath: /tmp
            - name: rendered-config
              mountPath: /bitnami/milvus/rendered-conf/
      containers:
        - name: milvus
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          args:
            - run
            - indexcoord
          env:
            - name: METRICS_PORT
              value: "9091"
          envFrom:
          ports:
            - containerPort: 19530
              name: grpc
            - containerPort: 9091
              name: http-metrics
          resources:
            limits: {}
            requests: {}
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          volumeMounts:
            - name: rendered-config
              mountPath: /opt/bitnami/milvus/configs
            - name: tmp
              mountPath: /tmp
            - name: tmp-milvus
              mountPath: /opt/bitnami/milvus/tmp
              # We are using a s3 backend, so this data dir is temporary
            - name: tmp-data-milvus
              mountPath: /bitnami/milvus/data
      volumes:
        - name: tmp-data-milvus
          emptyDir: {}
        - name: tmp-milvus
          emptyDir: {}
        - name: tmp
          emptyDir: {}
        - name: log-config
          configMap:
            name: myml2-milvus-log
        - name: config-common
          configMap:
            name: myml2-milvus
        - name: component-config-default
          configMap:
            name: myml2-milvus-index-coordinator
        - name: rendered-config
          emptyDir: {}
---
# Source: milvus/templates/index-node/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myml2-milvus-index-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: index-node
spec:
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: myml2
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: index-node
  template:
    metadata:
      annotations:
        checksum/common-config: 1d718420d1d135b8a6c5de63a3432db00d66753ff5e7c2c437683a14be472202
        checksum/common-config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      labels:
        app.kubernetes.io/instance: myml2
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: milvus
        app.kubernetes.io/version: 2.2.14
        helm.sh/chart: milvus-5.0.0
        app.kubernetes.io/part-of: milvus
        app.kubernetes.io/component: index-node
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: myml2
                    app.kubernetes.io/name: milvus
                    app.kubernetes.io/component: index-node
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
        - name: wait-for-etcd
          image: docker.io/bitnami/os-shell:11-debian-11-r91
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              etcd_hosts=(
                "http://myml2-etcd:2379"
              )
        
              check_etcd() {
                  local -r etcd_host="${1:-?missing etcd}"
                  local params_cert=""
        
                  if echo $etcd_host | grep https; then
                     params_cert="--cacert /bitnami/milvus/conf/cert/etcd/client/ca.crt --cert /bitnami/milvus/conf/cert/etcd/client/tls.crt --key /bitnami/milvus/conf/cert/etcd/client/tls.key"
                  fi
                  if [ ! -z  ]; then
                    params_cert=$params_cert" --pass "
                  fi
                  if curl --max-time 5 "${etcd_host}/version" $params_cert | grep etcdcluster; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${etcd_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_etcd $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-kafka
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1 
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              kafka_hosts=(
                "myml2-kafka"
              )
        
              check_kafka() {
                  local -r kafka_host="${1:-?missing kafka}"
                  if wait-for-port --timeout=5 --host=${kafka_host} --state=inuse 9092; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${kafka_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_kafka $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-s3
          image: docker.io/bitnami/os-shell:11-debian-11-r91
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              check_s3() {
                  local -r s3_host="${1:-?missing s3}"
                  if curl --max-time 5 "${s3_host}" | grep "RequestId"; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              host="myml2-minio"
        
              echo "Checking connection to $host"
              if retry_while "check_s3 $host"; then
                echo "Connected to $host"
              else
                echo "Error connecting to $host"
                exit 1
              fi
        
              echo "Connection success"
              exit 0
        # This init container renders and merges the Milvus configuration files.
        # We need to use a volume because we're working with ReadOnlyRootFilesystem
        - name: prepare-milvus
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              # Copy logging configuration
              cp /bitnami/milvus/conf/log/glog.conf /bitnami/milvus/rendered-conf/glog.conf
              # Build final milvus.yaml with the sections of the different files
              find /bitnami/milvus/conf -type f -name *.yaml -print0 | sort -z | xargs -0 yq eval-all '. as $item ireduce ({}; . * $item )' /opt/bitnami/milvus/configs/milvus.yaml > /bitnami/milvus/rendered-conf/pre-render-config_00.yaml
              # HACK: In order to enable Kafka we need to remove all Pulsar settings from the configuration file
              # https://github.com/milvus-io/milvus/blob/master/configs/milvus.yaml#L110
              yq 'del(.pulsar)' /bitnami/milvus/rendered-conf/pre-render-config_00.yaml > /bitnami/milvus/rendered-conf/pre-render-config_01.yaml
              render-template /bitnami/milvus/rendered-conf/pre-render-config_01.yaml > /bitnami/milvus/rendered-conf/milvus.yaml
              rm /bitnami/milvus/rendered-conf/pre-render-config*
              chmod 644 /bitnami/milvus/rendered-conf/milvus.yaml
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MILVUS_KAFKA_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: myml2-kafka-user-passwords
                  key: system-user-password
            - name: MILVUS_S3_ACCESS_ID
              valueFrom:
                secretKeyRef:
                  name: myml2-minio
                  key: root-user
            - name: MILVUS_S3_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: myml2-minio
                  key: root-password
          envFrom:
          volumeMounts:
            - name: log-config
              mountPath: /bitnami/milvus/conf/log
            - name: config-common
              mountPath: /bitnami/milvus/conf/00_default
            - name: component-config-default
              mountPath: /bitnami/milvus/conf/02_component_default
            - name: tmp
              mountPath: /tmp
            - name: rendered-config
              mountPath: /bitnami/milvus/rendered-conf/
      containers:
        - name: milvus
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          args:
            - run
            - indexnode
          env:
            - name: METRICS_PORT
              value: "9091"
          envFrom:
          ports:
            - containerPort: 19530
              name: grpc
            - containerPort: 9091
              name: http-metrics
          resources:
            limits: {}
            requests: {}
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          volumeMounts:
            - name: rendered-config
              mountPath: /opt/bitnami/milvus/configs
            - name: tmp
              mountPath: /tmp
            - name: tmp-milvus
              mountPath: /opt/bitnami/milvus/tmp
              # We are using a s3 backend, so this data dir is temporary
            - name: tmp-data-milvus
              mountPath: /bitnami/milvus/data
      volumes:
        - name: tmp-data-milvus
          emptyDir: {}
        - name: tmp-milvus
          emptyDir: {}
        - name: tmp
          emptyDir: {}
        - name: log-config
          configMap:
            name: myml2-milvus-log
        - name: config-common
          configMap:
            name: myml2-milvus
        - name: component-config-default
          configMap:
            name: myml2-milvus-index-node
        - name: rendered-config
          emptyDir: {}
---
# Source: milvus/templates/proxy/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myml2-milvus-proxy
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: proxy
spec:
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: myml2
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: proxy
  template:
    metadata:
      annotations:
        checksum/common-config: 1d718420d1d135b8a6c5de63a3432db00d66753ff5e7c2c437683a14be472202
        checksum/common-config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      labels:
        app.kubernetes.io/instance: myml2
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: milvus
        app.kubernetes.io/version: 2.2.14
        helm.sh/chart: milvus-5.0.0
        app.kubernetes.io/part-of: milvus
        app.kubernetes.io/component: proxy
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: myml2
                    app.kubernetes.io/name: milvus
                    app.kubernetes.io/component: proxy
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
        - name: wait-for-etcd
          image: docker.io/bitnami/os-shell:11-debian-11-r91
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              etcd_hosts=(
                "http://myml2-etcd:2379"
              )
        
              check_etcd() {
                  local -r etcd_host="${1:-?missing etcd}"
                  local params_cert=""
        
                  if echo $etcd_host | grep https; then
                     params_cert="--cacert /bitnami/milvus/conf/cert/etcd/client/ca.crt --cert /bitnami/milvus/conf/cert/etcd/client/tls.crt --key /bitnami/milvus/conf/cert/etcd/client/tls.key"
                  fi
                  if [ ! -z  ]; then
                    params_cert=$params_cert" --pass "
                  fi
                  if curl --max-time 5 "${etcd_host}/version" $params_cert | grep etcdcluster; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${etcd_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_etcd $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-kafka
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1 
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              kafka_hosts=(
                "myml2-kafka"
              )
        
              check_kafka() {
                  local -r kafka_host="${1:-?missing kafka}"
                  if wait-for-port --timeout=5 --host=${kafka_host} --state=inuse 9092; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${kafka_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_kafka $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-s3
          image: docker.io/bitnami/os-shell:11-debian-11-r91
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              check_s3() {
                  local -r s3_host="${1:-?missing s3}"
                  if curl --max-time 5 "${s3_host}" | grep "RequestId"; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              host="myml2-minio"
        
              echo "Checking connection to $host"
              if retry_while "check_s3 $host"; then
                echo "Connected to $host"
              else
                echo "Error connecting to $host"
                exit 1
              fi
        
              echo "Connection success"
              exit 0
        # This init container renders and merges the Milvus configuration files.
        # We need to use a volume because we're working with ReadOnlyRootFilesystem
        - name: prepare-milvus
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              # Copy logging configuration
              cp /bitnami/milvus/conf/log/glog.conf /bitnami/milvus/rendered-conf/glog.conf
              # Build final milvus.yaml with the sections of the different files
              find /bitnami/milvus/conf -type f -name *.yaml -print0 | sort -z | xargs -0 yq eval-all '. as $item ireduce ({}; . * $item )' /opt/bitnami/milvus/configs/milvus.yaml > /bitnami/milvus/rendered-conf/pre-render-config_00.yaml
              # HACK: In order to enable Kafka we need to remove all Pulsar settings from the configuration file
              # https://github.com/milvus-io/milvus/blob/master/configs/milvus.yaml#L110
              yq 'del(.pulsar)' /bitnami/milvus/rendered-conf/pre-render-config_00.yaml > /bitnami/milvus/rendered-conf/pre-render-config_01.yaml
              render-template /bitnami/milvus/rendered-conf/pre-render-config_01.yaml > /bitnami/milvus/rendered-conf/milvus.yaml
              rm /bitnami/milvus/rendered-conf/pre-render-config*
              chmod 644 /bitnami/milvus/rendered-conf/milvus.yaml
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MILVUS_KAFKA_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: myml2-kafka-user-passwords
                  key: system-user-password
            - name: MILVUS_S3_ACCESS_ID
              valueFrom:
                secretKeyRef:
                  name: myml2-minio
                  key: root-user
            - name: MILVUS_S3_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: myml2-minio
                  key: root-password
          envFrom:
          volumeMounts:
            - name: log-config
              mountPath: /bitnami/milvus/conf/log
            - name: config-common
              mountPath: /bitnami/milvus/conf/00_default
            - name: component-config-default
              mountPath: /bitnami/milvus/conf/02_component_default
            - name: tmp
              mountPath: /tmp
            - name: rendered-config
              mountPath: /bitnami/milvus/rendered-conf/
      containers:
        - name: milvus
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          args:
            - run
            - proxy
          env:
            - name: METRICS_PORT
              value: "9091"
          envFrom:
          ports:
            - containerPort: 19530
              name: grpc
            - containerPort: 19529
              name: grpc-internal
            - containerPort: 9091
              name: http-metrics
          resources:
            limits: {}
            requests: {}
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          volumeMounts:
            - name: rendered-config
              mountPath: /opt/bitnami/milvus/configs
            - name: tmp
              mountPath: /tmp
            - name: tmp-milvus
              mountPath: /opt/bitnami/milvus/tmp
              # We are using a s3 backend, so this data dir is temporary
            - name: tmp-data-milvus
              mountPath: /bitnami/milvus/data
      volumes:
        - name: tmp-data-milvus
          emptyDir: {}
        - name: tmp-milvus
          emptyDir: {}
        - name: tmp
          emptyDir: {}
        - name: log-config
          configMap:
            name: myml2-milvus-log
        - name: config-common
          configMap:
            name: myml2-milvus
        - name: component-config-default
          configMap:
            name: myml2-milvus-proxy
        - name: rendered-config
          emptyDir: {}
---
# Source: milvus/templates/query-coordinator/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myml2-milvus-query-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: query-coordinator
spec:
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: myml2
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: query-coordinator
  template:
    metadata:
      annotations:
        checksum/common-config: 1d718420d1d135b8a6c5de63a3432db00d66753ff5e7c2c437683a14be472202
        checksum/common-config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      labels:
        app.kubernetes.io/instance: myml2
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: milvus
        app.kubernetes.io/version: 2.2.14
        helm.sh/chart: milvus-5.0.0
        app.kubernetes.io/part-of: milvus
        app.kubernetes.io/component: query-coordinator
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: myml2
                    app.kubernetes.io/name: milvus
                    app.kubernetes.io/component: query-coordinator
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
        - name: wait-for-etcd
          image: docker.io/bitnami/os-shell:11-debian-11-r91
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              etcd_hosts=(
                "http://myml2-etcd:2379"
              )
        
              check_etcd() {
                  local -r etcd_host="${1:-?missing etcd}"
                  local params_cert=""
        
                  if echo $etcd_host | grep https; then
                     params_cert="--cacert /bitnami/milvus/conf/cert/etcd/client/ca.crt --cert /bitnami/milvus/conf/cert/etcd/client/tls.crt --key /bitnami/milvus/conf/cert/etcd/client/tls.key"
                  fi
                  if [ ! -z  ]; then
                    params_cert=$params_cert" --pass "
                  fi
                  if curl --max-time 5 "${etcd_host}/version" $params_cert | grep etcdcluster; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${etcd_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_etcd $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-kafka
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1 
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              kafka_hosts=(
                "myml2-kafka"
              )
        
              check_kafka() {
                  local -r kafka_host="${1:-?missing kafka}"
                  if wait-for-port --timeout=5 --host=${kafka_host} --state=inuse 9092; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${kafka_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_kafka $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-s3
          image: docker.io/bitnami/os-shell:11-debian-11-r91
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              check_s3() {
                  local -r s3_host="${1:-?missing s3}"
                  if curl --max-time 5 "${s3_host}" | grep "RequestId"; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              host="myml2-minio"
        
              echo "Checking connection to $host"
              if retry_while "check_s3 $host"; then
                echo "Connected to $host"
              else
                echo "Error connecting to $host"
                exit 1
              fi
        
              echo "Connection success"
              exit 0
        # This init container renders and merges the Milvus configuration files.
        # We need to use a volume because we're working with ReadOnlyRootFilesystem
        - name: prepare-milvus
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              # Copy logging configuration
              cp /bitnami/milvus/conf/log/glog.conf /bitnami/milvus/rendered-conf/glog.conf
              # Build final milvus.yaml with the sections of the different files
              find /bitnami/milvus/conf -type f -name *.yaml -print0 | sort -z | xargs -0 yq eval-all '. as $item ireduce ({}; . * $item )' /opt/bitnami/milvus/configs/milvus.yaml > /bitnami/milvus/rendered-conf/pre-render-config_00.yaml
              # HACK: In order to enable Kafka we need to remove all Pulsar settings from the configuration file
              # https://github.com/milvus-io/milvus/blob/master/configs/milvus.yaml#L110
              yq 'del(.pulsar)' /bitnami/milvus/rendered-conf/pre-render-config_00.yaml > /bitnami/milvus/rendered-conf/pre-render-config_01.yaml
              render-template /bitnami/milvus/rendered-conf/pre-render-config_01.yaml > /bitnami/milvus/rendered-conf/milvus.yaml
              rm /bitnami/milvus/rendered-conf/pre-render-config*
              chmod 644 /bitnami/milvus/rendered-conf/milvus.yaml
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MILVUS_KAFKA_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: myml2-kafka-user-passwords
                  key: system-user-password
            - name: MILVUS_S3_ACCESS_ID
              valueFrom:
                secretKeyRef:
                  name: myml2-minio
                  key: root-user
            - name: MILVUS_S3_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: myml2-minio
                  key: root-password
          envFrom:
          volumeMounts:
            - name: log-config
              mountPath: /bitnami/milvus/conf/log
            - name: config-common
              mountPath: /bitnami/milvus/conf/00_default
            - name: component-config-default
              mountPath: /bitnami/milvus/conf/02_component_default
            - name: tmp
              mountPath: /tmp
            - name: rendered-config
              mountPath: /bitnami/milvus/rendered-conf/
      containers:
        - name: milvus
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          args:
            - run
            - querycoord
          env:
            - name: METRICS_PORT
              value: "9091"
          envFrom:
          ports:
            - containerPort: 19530
              name: grpc
            - containerPort: 9091
              name: http-metrics
          resources:
            limits: {}
            requests: {}
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          volumeMounts:
            - name: rendered-config
              mountPath: /opt/bitnami/milvus/configs
            - name: tmp
              mountPath: /tmp
            - name: tmp-milvus
              mountPath: /opt/bitnami/milvus/tmp
              # We are using a s3 backend, so this data dir is temporary
            - name: tmp-data-milvus
              mountPath: /bitnami/milvus/data
      volumes:
        - name: tmp-data-milvus
          emptyDir: {}
        - name: tmp-milvus
          emptyDir: {}
        - name: tmp
          emptyDir: {}
        - name: log-config
          configMap:
            name: myml2-milvus-log
        - name: config-common
          configMap:
            name: myml2-milvus
        - name: component-config-default
          configMap:
            name: myml2-milvus-query-coordinator
        - name: rendered-config
          emptyDir: {}
---
# Source: milvus/templates/query-node/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myml2-milvus-query-node
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: query-node
spec:
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: myml2
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: query-node
  template:
    metadata:
      annotations:
        checksum/common-config: 1d718420d1d135b8a6c5de63a3432db00d66753ff5e7c2c437683a14be472202
        checksum/common-config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      labels:
        app.kubernetes.io/instance: myml2
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: milvus
        app.kubernetes.io/version: 2.2.14
        helm.sh/chart: milvus-5.0.0
        app.kubernetes.io/part-of: milvus
        app.kubernetes.io/component: query-node
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: myml2
                    app.kubernetes.io/name: milvus
                    app.kubernetes.io/component: query-node
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
        - name: wait-for-etcd
          image: docker.io/bitnami/os-shell:11-debian-11-r91
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              etcd_hosts=(
                "http://myml2-etcd:2379"
              )
        
              check_etcd() {
                  local -r etcd_host="${1:-?missing etcd}"
                  local params_cert=""
        
                  if echo $etcd_host | grep https; then
                     params_cert="--cacert /bitnami/milvus/conf/cert/etcd/client/ca.crt --cert /bitnami/milvus/conf/cert/etcd/client/tls.crt --key /bitnami/milvus/conf/cert/etcd/client/tls.key"
                  fi
                  if [ ! -z  ]; then
                    params_cert=$params_cert" --pass "
                  fi
                  if curl --max-time 5 "${etcd_host}/version" $params_cert | grep etcdcluster; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${etcd_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_etcd $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-kafka
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1 
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              kafka_hosts=(
                "myml2-kafka"
              )
        
              check_kafka() {
                  local -r kafka_host="${1:-?missing kafka}"
                  if wait-for-port --timeout=5 --host=${kafka_host} --state=inuse 9092; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${kafka_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_kafka $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-s3
          image: docker.io/bitnami/os-shell:11-debian-11-r91
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              check_s3() {
                  local -r s3_host="${1:-?missing s3}"
                  if curl --max-time 5 "${s3_host}" | grep "RequestId"; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              host="myml2-minio"
        
              echo "Checking connection to $host"
              if retry_while "check_s3 $host"; then
                echo "Connected to $host"
              else
                echo "Error connecting to $host"
                exit 1
              fi
        
              echo "Connection success"
              exit 0
        # This init container renders and merges the Milvus configuration files.
        # We need to use a volume because we're working with ReadOnlyRootFilesystem
        - name: prepare-milvus
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              # Copy logging configuration
              cp /bitnami/milvus/conf/log/glog.conf /bitnami/milvus/rendered-conf/glog.conf
              # Build final milvus.yaml with the sections of the different files
              find /bitnami/milvus/conf -type f -name *.yaml -print0 | sort -z | xargs -0 yq eval-all '. as $item ireduce ({}; . * $item )' /opt/bitnami/milvus/configs/milvus.yaml > /bitnami/milvus/rendered-conf/pre-render-config_00.yaml
              # HACK: In order to enable Kafka we need to remove all Pulsar settings from the configuration file
              # https://github.com/milvus-io/milvus/blob/master/configs/milvus.yaml#L110
              yq 'del(.pulsar)' /bitnami/milvus/rendered-conf/pre-render-config_00.yaml > /bitnami/milvus/rendered-conf/pre-render-config_01.yaml
              render-template /bitnami/milvus/rendered-conf/pre-render-config_01.yaml > /bitnami/milvus/rendered-conf/milvus.yaml
              rm /bitnami/milvus/rendered-conf/pre-render-config*
              chmod 644 /bitnami/milvus/rendered-conf/milvus.yaml
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MILVUS_KAFKA_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: myml2-kafka-user-passwords
                  key: system-user-password
            - name: MILVUS_S3_ACCESS_ID
              valueFrom:
                secretKeyRef:
                  name: myml2-minio
                  key: root-user
            - name: MILVUS_S3_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: myml2-minio
                  key: root-password
          envFrom:
          volumeMounts:
            - name: log-config
              mountPath: /bitnami/milvus/conf/log
            - name: config-common
              mountPath: /bitnami/milvus/conf/00_default
            - name: component-config-default
              mountPath: /bitnami/milvus/conf/02_component_default
            - name: tmp
              mountPath: /tmp
            - name: rendered-config
              mountPath: /bitnami/milvus/rendered-conf/
      containers:
        - name: milvus
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          args:
            - run
            - querynode
          env:
            - name: METRICS_PORT
              value: "9091"
          envFrom:
          ports:
            - containerPort: 19530
              name: grpc
            - containerPort: 9091
              name: http-metrics
          resources:
            limits: {}
            requests: {}
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          volumeMounts:
            - name: rendered-config
              mountPath: /opt/bitnami/milvus/configs
            - name: tmp
              mountPath: /tmp
            - name: tmp-milvus
              mountPath: /opt/bitnami/milvus/tmp
              # We are using a s3 backend, so this data dir is temporary
            - name: tmp-data-milvus
              mountPath: /bitnami/milvus/data
      volumes:
        - name: tmp-data-milvus
          emptyDir: {}
        - name: tmp-milvus
          emptyDir: {}
        - name: tmp
          emptyDir: {}
        - name: log-config
          configMap:
            name: myml2-milvus-log
        - name: config-common
          configMap:
            name: myml2-milvus
        - name: component-config-default
          configMap:
            name: myml2-milvus-query-node
        - name: rendered-config
          emptyDir: {}
---
# Source: milvus/templates/root-coordinator/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myml2-milvus-root-coordinator
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: milvus
    app.kubernetes.io/version: 2.2.14
    helm.sh/chart: milvus-5.0.0
    app.kubernetes.io/part-of: milvus
    app.kubernetes.io/component: root-coordinator
spec:
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: myml2
      app.kubernetes.io/name: milvus
      app.kubernetes.io/part-of: milvus
      app.kubernetes.io/component: root-coordinator
  template:
    metadata:
      annotations:
        checksum/common-config: 1d718420d1d135b8a6c5de63a3432db00d66753ff5e7c2c437683a14be472202
        checksum/common-config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/config-extra: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      labels:
        app.kubernetes.io/instance: myml2
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: milvus
        app.kubernetes.io/version: 2.2.14
        helm.sh/chart: milvus-5.0.0
        app.kubernetes.io/part-of: milvus
        app.kubernetes.io/component: root-coordinator
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: myml2
                    app.kubernetes.io/name: milvus
                    app.kubernetes.io/component: root-coordinator
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
        - name: wait-for-etcd
          image: docker.io/bitnami/os-shell:11-debian-11-r91
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              etcd_hosts=(
                "http://myml2-etcd:2379"
              )
        
              check_etcd() {
                  local -r etcd_host="${1:-?missing etcd}"
                  local params_cert=""
        
                  if echo $etcd_host | grep https; then
                     params_cert="--cacert /bitnami/milvus/conf/cert/etcd/client/ca.crt --cert /bitnami/milvus/conf/cert/etcd/client/tls.crt --key /bitnami/milvus/conf/cert/etcd/client/tls.key"
                  fi
                  if [ ! -z  ]; then
                    params_cert=$params_cert" --pass "
                  fi
                  if curl --max-time 5 "${etcd_host}/version" $params_cert | grep etcdcluster; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${etcd_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_etcd $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-kafka
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1 
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              kafka_hosts=(
                "myml2-kafka"
              )
        
              check_kafka() {
                  local -r kafka_host="${1:-?missing kafka}"
                  if wait-for-port --timeout=5 --host=${kafka_host} --state=inuse 9092; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              for host in "${kafka_hosts[@]}"; do
                  echo "Checking connection to $host"
                  if retry_while "check_kafka $host"; then
                      echo "Connected to $host"
                  else
                      echo "Error connecting to $host"
                      exit 1
                  fi
              done
        
              echo "Connection success"
              exit 0
        - name: wait-for-s3
          image: docker.io/bitnami/os-shell:11-debian-11-r91
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              retry_while() {
                local -r cmd="${1:?cmd is missing}"
                local -r retries="${2:-12}"
                local -r sleep_time="${3:-5}"
                local return_value=1
        
                read -r -a command <<< "$cmd"
                for ((i = 1 ; i <= retries ; i+=1 )); do
                    "${command[@]}" && return_value=0 && break
                    sleep "$sleep_time"
                done
                return $return_value
              }
        
              check_s3() {
                  local -r s3_host="${1:-?missing s3}"
                  if curl --max-time 5 "${s3_host}" | grep "RequestId"; then
                     return 0
                  else
                     return 1
                  fi
              }
        
              host="myml2-minio"
        
              echo "Checking connection to $host"
              if retry_while "check_s3 $host"; then
                echo "Connected to $host"
              else
                echo "Error connecting to $host"
                exit 1
              fi
        
              echo "Connection success"
              exit 0
        # This init container renders and merges the Milvus configuration files.
        # We need to use a volume because we're working with ReadOnlyRootFilesystem
        - name: prepare-milvus
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - bash
            - -ec
            - |
              #!/bin/bash
              # Copy logging configuration
              cp /bitnami/milvus/conf/log/glog.conf /bitnami/milvus/rendered-conf/glog.conf
              # Build final milvus.yaml with the sections of the different files
              find /bitnami/milvus/conf -type f -name *.yaml -print0 | sort -z | xargs -0 yq eval-all '. as $item ireduce ({}; . * $item )' /opt/bitnami/milvus/configs/milvus.yaml > /bitnami/milvus/rendered-conf/pre-render-config_00.yaml
              # HACK: In order to enable Kafka we need to remove all Pulsar settings from the configuration file
              # https://github.com/milvus-io/milvus/blob/master/configs/milvus.yaml#L110
              yq 'del(.pulsar)' /bitnami/milvus/rendered-conf/pre-render-config_00.yaml > /bitnami/milvus/rendered-conf/pre-render-config_01.yaml
              render-template /bitnami/milvus/rendered-conf/pre-render-config_01.yaml > /bitnami/milvus/rendered-conf/milvus.yaml
              rm /bitnami/milvus/rendered-conf/pre-render-config*
              chmod 644 /bitnami/milvus/rendered-conf/milvus.yaml
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MILVUS_KAFKA_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: myml2-kafka-user-passwords
                  key: system-user-password
            - name: MILVUS_S3_ACCESS_ID
              valueFrom:
                secretKeyRef:
                  name: myml2-minio
                  key: root-user
            - name: MILVUS_S3_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: myml2-minio
                  key: root-password
          envFrom:
          volumeMounts:
            - name: log-config
              mountPath: /bitnami/milvus/conf/log
            - name: config-common
              mountPath: /bitnami/milvus/conf/00_default
            - name: component-config-default
              mountPath: /bitnami/milvus/conf/02_component_default
            - name: tmp
              mountPath: /tmp
            - name: rendered-config
              mountPath: /bitnami/milvus/rendered-conf/
      containers:
        - name: milvus
          image: docker.io/bitnami/milvus:2.3.3-debian-11-r1
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          args:
            - run
            - rootcoord
          env:
            - name: METRICS_PORT
              value: "9091"
          envFrom:
          ports:
            - containerPort: 19530
              name: grpc
            - containerPort: 9091
              name: http-metrics
          resources:
            limits: {}
            requests: {}
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /healthz
              port: http-metrics
          volumeMounts:
            - name: rendered-config
              mountPath: /opt/bitnami/milvus/configs
            - name: tmp
              mountPath: /tmp
            - name: tmp-milvus
              mountPath: /opt/bitnami/milvus/tmp
              # We are using a s3 backend, so this data dir is temporary
            - name: tmp-data-milvus
              mountPath: /bitnami/milvus/data
      volumes:
        - name: tmp-data-milvus
          emptyDir: {}
        - name: tmp-milvus
          emptyDir: {}
        - name: tmp
          emptyDir: {}
        - name: log-config
          configMap:
            name: myml2-milvus-log
        - name: config-common
          configMap:
            name: myml2-milvus
        - name: component-config-default
          configMap:
            name: myml2-milvus-root-coordinator
        - name: rendered-config
          emptyDir: {}
---
# Source: milvus/charts/etcd/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: myml2-etcd
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: etcd
    app.kubernetes.io/version: 3.5.10
    helm.sh/chart: etcd-9.6.0
    app.kubernetes.io/component: etcd
spec:
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/instance: myml2
      app.kubernetes.io/name: etcd
      app.kubernetes.io/component: etcd
  serviceName: myml2-etcd-headless
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: myml2
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: etcd
        app.kubernetes.io/version: 3.5.10
        helm.sh/chart: etcd-9.6.0
        app.kubernetes.io/component: etcd
      annotations:
        checksum/token-secret: 04c640915f61e182742fdf26bd2a68f2bef84684828a45bc2747e394b75e0433
    spec:
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: myml2
                    app.kubernetes.io/name: etcd
                    app.kubernetes.io/component: etcd
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      serviceAccountName: "default"
      containers:
        - name: etcd
          image: docker.io/bitnami/etcd:3.5.10-debian-11-r0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_STS_NAME
              value: "myml2-etcd"
            - name: ETCDCTL_API
              value: "3"
            - name: ETCD_ON_K8S
              value: "yes"
            - name: ETCD_START_FROM_SNAPSHOT
              value: "no"
            - name: ETCD_DISASTER_RECOVERY
              value: "no"
            - name: ETCD_NAME
              value: "$(MY_POD_NAME)"
            - name: ETCD_DATA_DIR
              value: "/bitnami/etcd/data"
            - name: ETCD_LOG_LEVEL
              value: "info"
            - name: ALLOW_NONE_AUTHENTICATION
              value: "yes"
            - name: ETCD_AUTH_TOKEN
              value: "jwt,priv-key=/opt/bitnami/etcd/certs/token/jwt-token.pem,sign-method=RS256,ttl=10m"
            - name: ETCD_ADVERTISE_CLIENT_URLS
              value: "http://$(MY_POD_NAME).myml2-etcd-headless.default.svc.cluster.local:2379,http://myml2-etcd.default.svc.cluster.local:2379"
            - name: ETCD_LISTEN_CLIENT_URLS
              value: "http://0.0.0.0:2379"
            - name: ETCD_INITIAL_ADVERTISE_PEER_URLS
              value: "http://$(MY_POD_NAME).myml2-etcd-headless.default.svc.cluster.local:2380"
            - name: ETCD_LISTEN_PEER_URLS
              value: "http://0.0.0.0:2380"
            - name: ETCD_INITIAL_CLUSTER_TOKEN
              value: "etcd-cluster-k8s"
            - name: ETCD_INITIAL_CLUSTER_STATE
              value: "new"
            - name: ETCD_INITIAL_CLUSTER
              value: "myml2-etcd-0=http://myml2-etcd-0.myml2-etcd-headless.default.svc.cluster.local:2380,myml2-etcd-1=http://myml2-etcd-1.myml2-etcd-headless.default.svc.cluster.local:2380,myml2-etcd-2=http://myml2-etcd-2.myml2-etcd-headless.default.svc.cluster.local:2380"
            - name: ETCD_CLUSTER_DOMAIN
              value: "myml2-etcd-headless.default.svc.cluster.local"
          envFrom:
          ports:
            - name: client
              containerPort: 2379
              protocol: TCP
            - name: peer
              containerPort: 2380
              protocol: TCP
          livenessProbe:
            exec:
              command:
                - /opt/bitnami/scripts/etcd/healthcheck.sh
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            exec:
              command:
                - /opt/bitnami/scripts/etcd/healthcheck.sh
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          lifecycle:
            preStop:
              exec:
                command:
                  - /opt/bitnami/scripts/etcd/prestop.sh
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /bitnami/etcd
            - name: etcd-jwt-token
              mountPath: /opt/bitnami/etcd/certs/token/
              readOnly: true
      volumes:
        - name: etcd-jwt-token
          secret:
            secretName: myml2-etcd-jwt-token
            defaultMode: 256
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: milvus/charts/kafka/templates/controller-eligible/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: myml2-kafka-controller
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: 3.6.0
    helm.sh/chart: kafka-26.3.1
    app.kubernetes.io/component: controller-eligible
    app.kubernetes.io/part-of: kafka
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: myml2
      app.kubernetes.io/name: kafka
      app.kubernetes.io/component: controller-eligible
      app.kubernetes.io/part-of: kafka
  serviceName: myml2-kafka-controller-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: myml2
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: kafka
        app.kubernetes.io/version: 3.6.0
        helm.sh/chart: kafka-26.3.1
        app.kubernetes.io/component: controller-eligible
        app.kubernetes.io/part-of: kafka
      annotations:
        checksum/configuration: 2407ceef95ba7172c446f9a869cbf477c9a00d28ef5f2768c279406b5debafe3
        checksum/passwords-secret: b72cccd51bd2ba3cd29e5d067c3272dc5bf4c65c0da71f510d41cbf3f3e18473
    spec:
      
      hostNetwork: false
      hostIPC: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: myml2
                    app.kubernetes.io/name: kafka
                    app.kubernetes.io/component: controller-eligible
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: myml2-kafka
      enableServiceLinks: true
      initContainers:
        - name: kafka-init
          image: docker.io/bitnami/kafka:3.6.0-debian-11-r1
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /bin/bash
          args:
            - -ec
            - |
              /scripts/kafka-init.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                    fieldPath: metadata.name
            - name: KAFKA_VOLUME_DIR
              value: "/bitnami/kafka"
            - name: KAFKA_MIN_ID
              value: "0"
            - name: KAFKA_CLIENT_USERS
              value: "user"
            - name: KAFKA_CLIENT_PASSWORDS
              valueFrom:
                secretKeyRef:
                  name: myml2-kafka-user-passwords
                  key: client-passwords
            - name: KAFKA_INTER_BROKER_USER
              value: "inter_broker_user"
            - name: KAFKA_INTER_BROKER_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: myml2-kafka-user-passwords
                  key: inter-broker-password
            - name: KAFKA_CONTROLLER_USER
              value: "controller_user"
            - name: KAFKA_CONTROLLER_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: myml2-kafka-user-passwords
                  key: controller-password
          volumeMounts:
            - name: data
              mountPath: /bitnami/kafka
            - name: kafka-config
              mountPath: /config
            - name: kafka-configmaps
              mountPath: /configmaps
            - name: kafka-secret-config
              mountPath: /secret-config
            - name: scripts
              mountPath: /scripts
            - name: tmp
              mountPath: /tmp
      containers:
        - name: kafka
          image: docker.io/bitnami/kafka:3.6.0-debian-11-r1
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: KAFKA_HEAP_OPTS
              value: "-Xmx1024m -Xms1024m"
            - name: KAFKA_KRAFT_CLUSTER_ID
              valueFrom:
                secretKeyRef:
                  name: myml2-kafka-kraft-cluster-id
                  key: kraft-cluster-id
          ports:
            - name: controller
              containerPort: 9093
            - name: client
              containerPort: 9092
            - name: interbroker
              containerPort: 9094
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: "controller"
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: "controller"
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /bitnami/kafka
            - name: logs
              mountPath: /opt/bitnami/kafka/logs
            - name: kafka-config
              mountPath: /opt/bitnami/kafka/config/server.properties
              subPath: server.properties
            - name: tmp
              mountPath: /tmp
      volumes:
        - name: kafka-configmaps
          configMap:
            name: myml2-kafka-controller-configuration
        - name: kafka-secret-config
          emptyDir: {}
        - name: kafka-config
          emptyDir: {}
        - name: tmp
          emptyDir: {}
        - name: scripts
          configMap:
            name: myml2-kafka-scripts
            defaultMode: 0755
        - name: logs
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: milvus/charts/minio/templates/provisioning-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: myml2-minio-provisioning
  namespace: "default"
  labels:
    app.kubernetes.io/instance: myml2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: minio
    app.kubernetes.io/version: 2023.11.1
    helm.sh/chart: minio-12.9.0
    app.kubernetes.io/component: minio-provisioning
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
spec: 
  parallelism: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: 2023.11.1
        helm.sh/chart: minio-12.9.0
        app.kubernetes.io/component: minio-provisioning
    spec:
      
      restartPolicy: OnFailure
      terminationGracePeriodSeconds: 0
      securityContext:
        fsGroup: 1001
      serviceAccountName: myml2-minio
      initContainers:
        - name: wait-for-available-minio
          image: docker.io/bitnami/minio:2023.11.1-debian-11-r0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - /bin/bash
            - -c
            - |-
              set -e;
              echo "Waiting for Minio";
              wait-for-port \
                --host=myml2-minio \
                --state=inuse \
                --timeout=120 \
                80;
              echo "Minio is available";
          resources:
            limits: {}
            requests: {}
      containers:
        - name: minio
          image: docker.io/bitnami/minio:2023.11.1-debian-11-r0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - /bin/bash
            - -c
            - >-
              set -e;
              echo "Start Minio provisioning";

              function attachPolicy() {
                local tmp=$(mc admin $1 info provisioning $2 | sed -n -e 's/^Policy.*: \(.*\)$/\1/p');
                IFS=',' read -r -a CURRENT_POLICIES <<< "$tmp";
                if [[ ! "${CURRENT_POLICIES[*]}" =~ "$3" ]]; then
                  mc admin policy attach provisioning $3 --$1=$2;
                fi;
              };

              function detachDanglingPolicies() {
                local tmp=$(mc admin $1 info provisioning $2 | sed -n -e 's/^Policy.*: \(.*\)$/\1/p');
                IFS=',' read -r -a CURRENT_POLICIES <<< "$tmp";
                IFS=',' read -r -a DESIRED_POLICIES <<< "$3";
                for current in "${CURRENT_POLICIES[@]}"; do
                  if [[ ! "${DESIRED_POLICIES[*]}" =~ "${current}" ]]; then
                    mc admin policy detach provisioning $current --$1=$2;
                  fi;
                done;
              }

              function addUsersFromFile() {
                local username=$(grep -oP '^username=\K.+' $1);
                local password=$(grep -oP '^password=\K.+' $1);
                local disabled=$(grep -oP '^disabled=\K.+' $1);
                local policies_list=$(grep -oP '^policies=\K.+' $1);
                local set_policies=$(grep -oP '^setPolicies=\K.+' $1);

                mc admin user add provisioning "${username}" "${password}";

                IFS=',' read -r -a POLICIES <<< "${policies_list}";
                for policy in "${POLICIES[@]}"; do
                  attachPolicy user "${username}" "${policy}";
                done;
                if [ "${set_policies}" == "true" ]; then
                  detachDanglingPolicies user "${username}" "${policies_list}";
                fi;

                local user_status="enable";
                if [[ "${disabled}" != "" && "${disabled,,}" == "true" ]]; then
                  user_status="disable";
                fi;

                mc admin user "${user_status}" provisioning "${username}";
              };
              mc alias set provisioning $MINIO_SCHEME://myml2-minio:80 $MINIO_ROOT_USER $MINIO_ROOT_PASSWORD;

              mc admin service restart provisioning;
              
              mc anonymous set download provisioning/milvus;

              echo "End Minio provisioning";
          env:
            - name: MINIO_SCHEME
              value: "http"
            - name: MINIO_ROOT_USER
              valueFrom:
                secretKeyRef:
                  name: myml2-minio
                  key: root-user
            - name: MINIO_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: myml2-minio
                  key: root-password
          envFrom:
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: minio-provisioning
              mountPath: /etc/ilm
      volumes:
        - name: minio-provisioning
          configMap:
            name: myml2-minio-provisioning
